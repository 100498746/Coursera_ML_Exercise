# 程序设计练习4：神经网络学习
## 介绍
&#160;&#160;&#160;&#160;在这个练习中，你将实现神经网络的反向传播算法并将其应用到手写数字识别的任务中。在开始编程之前，我们强烈建议你线观看视频课程并完成相关主题的复习问题。

&#160;&#160;&#160;&#160;要开始练习，您需要下载起始代码并将其内容解压缩到您希望完成练习的目录中。如果需要，请在开始本练习之前使用octave/matlab中的cd命令更改到此目录。

&#160;&#160;&#160;&#160;你也可以在课程网站的“环境设置说明”中找到安装octave/matlab的说明。

### 本练习包含的文件
    ex4.m - 指导你完成练习的Octave/MATLAB脚本
    ex4data1.mat - 手写数字识别的训练集
    ex4weights.mat - exercise 4的神经网络参数
    submit.m -提交作业的脚本
    displayData.m - 可视化数据集的脚本
    fmincg.m - 最小化函数 (类似于fminunc)
    sigmoid.m - S函数
    computeNumericalGradient.m - 数值计算梯度
    checkNNGradients.m - 帮助检查你的梯度的代码
    debugInitializeWeights.m - 初始化权重的函数
    predict.m - 神经网络预测函数
    [*] sigmoidGradient.m - 计算S函数的梯度
    [*] randInitializeWeights.m - 随机初始化权重
    [*] nnCostFunction.m - 神经网络代价函数
    
    * 表示你需要完成的文件

&#160;&#160;&#160;&#160;在本练习中，你将使用ex4.m脚本。该脚本为题目设置数据集并且调用你编写的函数。你不需要修改这些脚本，只需要按照作业说明定义其他函数。

### 在哪里寻求帮助
&#160;&#160;&#160;&#160;本课程的练习使用非常适合数值计算的高级编程语言Octave或MATLAB。如果你没有安装Octave或MATLAB，请参阅课程网站上“环境设置说明”中的安装说明。

&#160;&#160;&#160;&#160;在Octave/MATLAB命令行中输入help紧跟函数名称会显示内建的函数说明。比如输入help plot会显示绘图函数的帮助信息。更多Octave和MATLAB的函数说明请在[Octave官网](https://octave.org/doc/interpreter/)和[MATLAB官网](https://www.mathworks.com/help/matlab/?refresh=true)查阅。

&#160;&#160;&#160;&#160;我们也非常鼓励使用在线讨论与其他学生讨论练习。但是，不要查看任何源代码或与他人共享源代码。

---

## 1、神经网络
&#160;&#160;&#160;&#160;在前面的练习中，你实现了神经网络的前馈传播并根据我们提供的权重使用它来预测手写数字的识别。在本练习中，你将实现反向传播算法来学习神经网络的参数。

&#160;&#160;&#160;&#160;ex4.m脚本将指导你逐步完成本练习。

### 1.1 数据可视化
&#160;&#160;&#160;&#160;在ex4.m的第一部分，代码加载数据集并通过调用displayData函数将其展示在一个二维图中（图1）。
<center><img src="https://note.youdao.com/yws/api/personal/file/WEB517f53c38209ce110e264699aa0d5067?method=download&shareKey=d63aec370857b533e28f023f251998fd" /></center>
<center><h6>Figure 1: Examples from the dataset</h6></center>

&#160;&#160;&#160;&#160;这和你在前面的练习中使用的是同一个数据集。在ex3data1.mat中有5000个训练样本，每一个训练样本是一个20px * 20px的灰度数字图像。
每个像素由一个浮点数表示，该浮点数表示该位置的灰度强度。20×20像素的网格被“展示”成一个400维的向量。这些训练示例中的每一个都变成了数据矩阵X中的一行。这样就给了我们一个5000×400矩阵X，其中每一行都是手写数字图像的训练示例。

<center><img src="https://note.youdao.com/yws/api/personal/file/WEB315e507ecc41d536d2b97d50eaa38c2c?method=download&shareKey=ac587a9edd8d9a05f04a6265dce03374" /></center>

&#160;&#160;&#160;&#160;训练集的第二部分是包含训练集标签的5000维向量y，为了使它与Octave/MATLAB索引更加兼容，在没有零索引的情况下，我们将数字0映射到值10。因此，数字“0”被标记为“10”，而“1”到“9”的数字按其自然顺序被标记为“1”到“9”。

### 1.2 模型表示
&#160;&#160;&#160;&#160;我们的神经网络如图2所示，总共有3层——一个输入层、一个隐藏层和一个输出层。回想一下，我们的输入是数字图像的像素值。由于图像大小20×20，因此给了我们400个输入层单位（不包括额外的偏差项）。和之前一样，训练数据将被加载到变量X和y中。

&#160;&#160;&#160;&#160;我们 已经为你提供了我们训练的一组神经网络参数（θ(1),θ(2)）。他们存在ex4weights.mat中并可以使用y ex4.m加载到Theta1和Theta2中。参数具有针对神经网络的大小，其在第二层中具有25个单元并且具有10个输出单元（对应于10个数字类）。

```
% Load saved matrices from file
load('ex3weights.mat');
% The matrices Theta1 and Theta2 will now be in your Octave
% environment
% Theta1 has size 25 x 401
% Theta2 has size 10 x 26
```
<center><img src="https://note.youdao.com/yws/api/personal/file/WEBdba65583247ba093a8be6c3a07093f6c?method=download&shareKey=2d6cedef95e5129f3c30915cc19c322f" width="60%"/></center>
<center><h6>Figure 2: Neural network model</h6></center>

### 1.3 前馈和代价函数
&#160;&#160;&#160;&#160;现在你将实现神经网络的代价函数和梯度。首先完成nnCostFunction.m中的代码返回代价。

&#160;&#160;&#160;&#160;回想一下神经网络的代价（没有正则化）
<center><img src="https://note.youdao.com/yws/api/personal/file/WEBd26f03a2d260deddadb07c80bdac3bf7?method=download&shareKey=bfd556d0f60e881f86dcca50b6d72c64" width="80%" /></center>
其中hθ(x(i))如图2所示计算，K=10是可能标签的总数。注意，hθ(x(i))k = a(3)是第k个输出单元的激活（输出值）。另外，回想一下，原来的标签(在变量y中)是1,2,…, 10，为了训练神经网络，我们需要将标签重新编码为只包含0或1值的向量，像这样
<center><img src="https://note.youdao.com/yws/api/personal/file/WEBb82f4eb62d7d113e8823f9c23ff196c5?method=download&shareKey=47a78f78bf517fa1d6a1bd331f49446c" /></center>

&#160;&#160;&#160;&#160;例如，如果xi是一个数字5的图像，那么对应的yi（你在代价函数中使用的）应该是一个10维的向量，其中y5=1，其他元素为0。

&#160;&#160;&#160;&#160;你应该实现前馈计算，该计算为每个样本计算hθ(x(i))并且对所有样本的代价求和。**你的代码应该适用于具有任意数量标签任意规模的数据集**（你可以认为总会有至少K≥3个标签）。

> **实现注意**：矩阵X以行为单位包含样本（例如X(i, :)是第i个训练样本，表示为一个n * 1的向量）。当你在nnCostFunction.m中写代码时，你需要在X矩阵中添加全为1的列。神经网络中每个单元的参数用Theta1和Theta2表示为一行。具体地说，Theta1的第一行对应于第二层中的第一个隐藏单元。您可以在示例中使用for循环来计算代价。

&#160;&#160;&#160;&#160;一旦你完成了，ex4.m将使用加载的Theta1和Theta2参数调用nnCostFunction.m。你应该看到成本大约是0.287629。

&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;*==你现在应该提交答案==*

### 1.4 正则化的代价函数
&#160;&#160;&#160;&#160;正则化的神经网络的代价函数如下：
<center><img src="https://note.youdao.com/yws/api/personal/file/WEBf0704daf8c5ec8727b41adcac9d2d5c7?method=download&shareKey=4170b6684354c0486a10452cbc5fe569" width="80%" /></center>
&#160;&#160;&#160;&#160;假设神经网络有三层——一个输入层，一个隐藏层和一个输出层。但是，你的代码应该适用于任意数量的输入单元、隐藏单元和输出单元。虽然为了清楚起见，我们已明确列出了Θ(1)和Θ(2)的上述索引，但请注意，你的代码应使用任何大小的Θ(1)和Θ(2)。

&#160;&#160;&#160;&#160;请注意，你不应该将与偏差对应的那一项正则化。对矩阵Theta1和矩阵Theta2来说，所对应的就是它们的第一列。你现在应该正则化你的代价函数。注意，你可以先使用现有的nnCostFunction.m计算非正则化代价函数J然后为其添加正则项。

&#160;&#160;&#160;&#160;一旦完成，ex4.m将使用加载了Theta1和Theta2的参数集调用你的nnCostFunction，并且λ=1。你应该看到成本约为0.383770。

&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;*==你现在应该提交答案==*

## 2、反向传播
&#160;&#160;&#160;&#160;在这部分练习中，你将使用反向传播算法为神经网络代价函数计算梯度。你需要完成nnCostFunction.m，以便返回适当的梯度值。 计算出梯度后，您将能够通过使用高级优化器（如fmincg）最小化成本函数J(Θ)来训练神经网络。

&#160;&#160;&#160;&#160;你将首先实现反向传播算法来计算（未正则化的）神经网络的参数的梯度。当你验证了你的梯度计算（非正则化的例子）是正确的之后，你将实现正则化神经网络的梯度。


### 2.1 Sigmoid梯度
&#160;&#160;&#160;&#160;为了帮你开始这部分练习，你第一个需要实现的是sigmoid梯度函数。sigmoid函数的梯度计算公式如下：
<center><img src="https://note.youdao.com/yws/api/personal/file/WEB5bfdc23264bd085bdd435857a15043b6?method=download&shareKey=01b75a7f6e48b568856007df9d3b9441" width="60%" /></center>

&#160;&#160;&#160;&#160;当你结束之后，尝试在Octave/matlab命令行中通过调用sigmoidGradient(z)来测试一些值。对于z的比较大的值(包括正值和负值)，梯度应该接近于0。当z=0时，梯度应该正好是0.25。你的代码还应该处理向量和矩阵。对于矩阵，函数应该对每个元素执行sigmoid梯度函数。

&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;*==你现在应该提交答案==*


### 2.2 随机初始化
&#160;&#160;&#160;&#160;在训练神经网络时，随机初始化参数以进行对称破坏非常重要。随机初始化比较好的一个策略是在范围[-E，E]中随机均匀地为θ选择值。你应该让E=0.12。这个范围的值确保参数保持在较小的范围内，并使学习更有效。

&#160;&#160;&#160;&#160;你的工作是完成randInitializeWeights.m中的代码为θ初始化权重，修改文件并加入以下代码：

```
    % Randomly initialize the weights to small values
    epsilon init = 0.12;
    W = rand(L out, 1 + L in) * 2 * epsilon init − epsilon init;
```
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;*==这部分练习不需要提交任何代码==*

### 2.3 反向传播
<center><img src="https://note.youdao.com/yws/api/personal/file/WEBf54157749c3a278d6bf089fbf0c0a3bc?method=download&shareKey=bfec3568c12419eeb870eabc973e195e" /></center>
<center><h6>Figure 3: Backpropagation Updates.</h6></center>
&#160;&#160;&#160;&#160;现在你将实现反向传播算法。回想一下，反向传播算法背后的直观理解如下。
给定一个训练样本(x, y)，我们将首先运行一个“向前传递”来计算整个网络中的所有激活，包括假设函数的输出值。然后我们为第l层的每个节点（神经元）j计算误差δ(l)j（l为上标，j为下标），它来权衡该节点对输出中的任何错误“负责”的程度。

&#160;&#160;&#160;&#160;对于输出节点，我们可以直接测量网络激活和真实目标值之间的差异，并使用它来定义δ(3)j（因为第3层是输出层）。对于隐藏单元，你将根据（l+1）层中节点的误差项的加权平均值计算δ(l)j。

&#160;&#160;&#160;&#160;详细一点就是，这是反向传播算法（图3）。你应该在一个循环中实现步骤1到4，该循环每次处理一个样本。具体来说，你应该为t=1:m实现一个for循环并在for循环中加入下面的四个步骤，第t次迭代在第t个训练样本(x(t), y(t))上执行计算。步骤5将累积梯度除以m得到神经网络代价函数的梯度。

&#160;&#160;&#160;&#160;1、将输入层的值（a(1)）设置为第t个训练样本x(t)。执行一个前馈传递（图2）为第二层和第三层计算激活（z(2), a(2), z(3), a(3)）。请注意，你需要添加+1项以确保层a(1)和a(2)的激活矢量也包括偏置单元。在Octave/MATLAB中，如果a_1是一个列向量，为列向量添加1对应aa_1 = [1; a_1]。

&#160;&#160;&#160;&#160;2、对在第三层（输出层）的每个输出单元k，设置
<center><img src="https://note.youdao.com/yws/api/personal/file/WEB53e992d2b6b7fba85190a39b5297d39a?method=download&shareKey=964852bb8a212b450e76a45e8c2d13fc" /></center>
其中yk∈ {0,1}，指示当前训练样本是属于类k（yk = 1），还是属于不同的类（yk = 0）。
你可能会发现逻辑数组对此任务有帮助（在之前的编程练习中进行了解释）。

&#160;&#160;&#160;&#160;3、对于隐藏层l=2，设置
